{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-text-mining/resources/d9pwm) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Introduction to NLTK\n",
    "\n",
    "In part 1 of this assignment you will use nltk to explore the Herman Melville novel Moby Dick. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Analyzing Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/austin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/austin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/austin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# If you would like to work with the raw text you can use 'moby_raw'\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "    \n",
    "# If you would like to work with the novel in nltk.Text format you can use 'text1'\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "How many tokens (words and punctuation symbols) are in text1?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_one():\n",
    "    \n",
    "    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "How many unique tokens (unique words and punctuation) does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_two():\n",
    "    \n",
    "    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "After lemmatizing the verbs, how many unique tokens does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
    "\n",
    "    return len(set(lemmatized))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07393408041154652"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def answer_one():\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() #instantiate WordNetLemmatizer\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in text1] # use list comprehension to lemmatize each work in text1\n",
    "    \n",
    "    total = len(lemmatized) #calculate the total lemmas\n",
    "    unique = len(set(lemmatized)) #calclulate the number of unique lemmas\n",
    "\n",
    "    lexical_diversity = unique/total #calculate the ratio of unique lemmas to total lemmas      \n",
    "    \n",
    "    return lexical_diversity\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What percentage of tokens is 'whale'or 'Whale'?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41248755087477157"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def answer_two():\n",
    "\n",
    "    tokens = nltk.word_tokenize(moby_raw)  #tokenize moby_raw\n",
    "    dist = nltk.FreqDist(tokens) #calculate the distribution of tokens\n",
    "    \n",
    "    whale_freq = dist.freq(\"whale\")\n",
    "    Whale_freq = dist.freq(\"Whale\")\n",
    "    #access the frequency distributions for 'whale' and 'Whale'\n",
    "    \n",
    "    total_freq = (whale_freq + Whale_freq)*100 \n",
    "    #add the distributions, multiply by 100 for a percentage\n",
    "    \n",
    "    return total_freq\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "\n",
    "*This function should return a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7306),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978),\n",
       " ('his', 2459),\n",
       " ('it', 2196),\n",
       " ('I', 2113),\n",
       " ('!', 1767),\n",
       " ('is', 1722),\n",
       " ('--', 1713),\n",
       " ('with', 1659),\n",
       " ('he', 1658),\n",
       " ('was', 1639),\n",
       " ('as', 1620)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "    \n",
    "    tokens = nltk.word_tokenize(moby_raw) #tokenize moby_raw\n",
    "    \n",
    "#     top_20 = dist.pformat(maxlen=20)\n",
    "    \n",
    "#     top_20_split = top_20.split(',')\n",
    "  \n",
    "#     word = []\n",
    "#     count = []\n",
    "    \n",
    "#     for i,value in enumerate(top_20_split):\n",
    "#         word.append(top_20_split[i].split(':')[0].replace(\"'\",\"\").lstrip().rstrip())\n",
    "        \n",
    "#     word.pop(0)\n",
    "#     word.insert(0,',')\n",
    "#     word.pop(21)\n",
    "#     word.pop(1)\n",
    "#     #word.pop(19)\n",
    "#     #word.insert(19,\"''\")\n",
    "    \n",
    "#     for i in range(len(top_20_split)-2):\n",
    "#         count.append(int(top_20_split[i+1].split(':')[1].lstrip().rstrip()))\n",
    "        \n",
    "#     merged_list= list(zip(word,count))\n",
    "\n",
    "##   Took the scenic route the first go around\n",
    "    \n",
    "    top_20 = (nltk.FreqDist(tokens)\n",
    "              .most_common(20)) #calculate the distribution of tokens and return the top 20\n",
    "    \n",
    "    return top_20\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "\n",
    "*This function should return an alphabetically sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain',\n",
       " 'Pequod',\n",
       " 'Queequeg',\n",
       " 'Starbuck',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'himself',\n",
       " 'little',\n",
       " 'seemed',\n",
       " 'should',\n",
       " 'though',\n",
       " 'through',\n",
       " 'whales',\n",
       " 'without']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "    \n",
    "    tokens = nltk.word_tokenize(moby_raw) #tokenize moby_raw\n",
    "    \n",
    "    dist = nltk.FreqDist(tokens) #calculate the distribution of tokens\n",
    "    \n",
    "    fivechars_onefiftytimes = [w for w in dist if len(w) > 5 and dist[w] > 150]\n",
    "    # use list comprehension to return tokens greater than length 5 that have a frequency greater than 150\n",
    "    \n",
    "    fivechars_onefiftytimes.sort()\n",
    "    \n",
    "    return fivechars_onefiftytimes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Find the longest word in text1 and that word's length.\n",
    "\n",
    "*This function should return a tuple `(longest_word, length)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_five():\n",
    "    \n",
    "    lemmatizer = nltk.WordNetLemmatizer() #instantiate WordNetLemmatizer\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in text1] #create a list of lemmas using the lemmatizer instance\n",
    "    \n",
    "    dist = nltk.FreqDist(lemmatized) #calculate the distribution of lemmas\n",
    "    \n",
    "#     first_answer = ([w for w in lemmatized if len(w) > 22][0],\n",
    "#                     len([w for w in lemmatized if len(w) > 22][0]))\n",
    "#     #guess and check method using a list comprehension\n",
    "    \n",
    "    longest_word_length = 0\n",
    "    longest_word = \"\"\n",
    "    \n",
    "    for i, word in enumerate(lemmatized):\n",
    "        if len(word) > longest_word_length:\n",
    "            longest_word_length = len(word)\n",
    "            longest_word = word\n",
    "        else:\n",
    "            pass\n",
    "    #loop over list of lemmas and store the running longest word length along with the actual word\n",
    "\n",
    "    answer = (longest_word, longest_word_length)        \n",
    "    \n",
    "    return answer\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What unique words have a frequency of more than 2000? What is their frequency?\n",
    "\n",
    "\"Hint:  you may want to use `isalpha()` to check if the token is a word and not punctuation.\"\n",
    "\n",
    "*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13715, 'the'),\n",
       " (6513, 'of'),\n",
       " (6010, 'and'),\n",
       " (4545, 'a'),\n",
       " (4515, 'to'),\n",
       " (3908, 'in'),\n",
       " (2978, 'that'),\n",
       " (2459, 'his'),\n",
       " (2196, 'it'),\n",
       " (2113, 'I')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():\n",
    "    \n",
    "    tokens = nltk.word_tokenize(moby_raw) #instantiate WordNetLemmatizer\n",
    "    dist = nltk.FreqDist(tokens) #calculate the distribution of lemmas\n",
    "    \n",
    "    token = [w for w in dist if w.isalpha() and dist[w] > 2000] \n",
    "    #use list comprehension to return tokens that are words and have a frequency greater than 2000\n",
    "    count = [dist[w] for w in dist if w.isalpha() and dist[w] > 2000]\n",
    "    #use list comprehension to return the count for the tokens returned in the above list comprehension\n",
    "    \n",
    "    combined = list(zip(count, token)) #combine the two lists\n",
    "    combined.sort(reverse = True, key = lambda x: x[0]) #sort the list by the first entry in the tuple\n",
    "         \n",
    "    return combined\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What is the average number of tokens per sentence?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.886926512383273"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_seven():\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(moby_raw) #instantiate a sentence tokenizer instance\n",
    "    text_sentences = nltk.Text(sentences) \n",
    "\n",
    "    counter = 0\n",
    "    sum_total = 0\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sum_total += len(nltk.word_tokenize(sentence)) \n",
    "        #loop through sentences and calculate a running total of sentence lengths\n",
    "        counter += 1\n",
    "    \n",
    "    average = sum_total/counter \n",
    "    #return sum of sentences lengths over count of sentences; average sentence length\n",
    "\n",
    "    return average\n",
    "\n",
    "answer_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "\n",
    "*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 32729), ('IN', 28663), ('DT', 25879), (',', 19204), ('JJ', 17613)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_eight():\n",
    "    \n",
    "    tokens = nltk.word_tokenize(moby_raw) #instantiate a sentence tokenizer instance\n",
    "    tags = nltk.pos_tag(tokens) #create list of tokens with their respective parts of speech\n",
    "    \n",
    "    pos_tag = []\n",
    "    for i,tag in enumerate(tags):\n",
    "        pos_tag.append(tag[1])\n",
    "    #create a list of all parts of speech in moby raw        \n",
    "    \n",
    "    dist = nltk.FreqDist(pos_tag) #calculate the frequency distribution of the pos tags\n",
    "    freq = [dist[w] for w in dist] #create a list of frequency counts from dist\n",
    "    pos_tag_consolidated = [w for w in dist] #create a list of parts of speech from dist   \n",
    "    zipped = list(zip(pos_tag_consolidated,freq)) \n",
    "    \n",
    "    zipped.sort(reverse = True, key = lambda x:x[1])\n",
    "    top_5 = zipped[:5]\n",
    "    \n",
    "    return top_5\n",
    "\n",
    "answer_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Spelling Recommender\n",
    "\n",
    "For this part of the assignment you will create three different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.\n",
    "\n",
    "*Each of the three different recommenders will use a different distance measure (outlined below).\n",
    "\n",
    "Each of the recommenders should provide recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/austin/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "correct_spellings = words.words() # list of correctly spelled words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpulent :  0.6\n",
      "indecence :  0.6666666666666666\n",
      "validate :  0.5555555555555556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpulent', 'indecence', 'validate']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    \n",
    "    entries = entries   \n",
    "    jaccard_list = []\n",
    "    for i in range(len(entries)):\n",
    "        jaccard_dict = {}\n",
    "        for j,word in enumerate(correct_spellings):\n",
    "            jaccard_dict[word] = nltk.jaccard_distance(set(nltk.ngrams(word,n=3)),\n",
    "                                                      set(nltk.ngrams(entries[i],n=3)))\n",
    "\n",
    "        jaccard_list.append(jaccard_dict)\n",
    "    #put dictionaries in a list\n",
    "    \n",
    "    for i in range(len(jaccard_list)):\n",
    "        jaccard_list[i] = {k: v for k, v in sorted(jaccard_list[i].items(), key = lambda item: item[1])}\n",
    "    #sort each dictionary in jaccard list by the jaccard distance\n",
    "    \n",
    "    answer = []       \n",
    "    for i in range(len(jaccard_list)):\n",
    "        for key in jaccard_list[i]:\n",
    "            if key.startswith(entries[i][0]):\n",
    "                print(key,\": \",jaccard_list[i][key])\n",
    "                answer.append(key)\n",
    "                break\n",
    "    #return the word with the smallest jaccard distance that starts with the same letter as the misspelled word   \n",
    "         \n",
    "    return answer\n",
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cormus :  0.7142857142857143\n",
      "incendiary :  0.75\n",
      "valid :  0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cormus', 'incendiary', 'valid']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "        \n",
    "    entries = entries\n",
    "    \n",
    "    jaccard_list = []\n",
    "    for i in range(len(entries)):\n",
    "        jaccard_dict = {}\n",
    "        for j,word in enumerate(correct_spellings):\n",
    "            jaccard_dict[word] = nltk.jaccard_distance(set(nltk.ngrams(word,n=4)),\n",
    "                                                      set(nltk.ngrams(entries[i],n=4)))\n",
    "        jaccard_list.append(jaccard_dict)\n",
    "        \n",
    "    for i in range(len(jaccard_list)):\n",
    "        jaccard_list[i] = {k: v for k, v in sorted(jaccard_list[i].items(), key = lambda item: item[1])}\n",
    "\n",
    "    answer = []       \n",
    "    for i in range(len(jaccard_list)):\n",
    "        for key in jaccard_list[i]:\n",
    "            if key.startswith(entries[i][0]):\n",
    "                print(key,\": \",jaccard_list[i][key])\n",
    "                answer.append(key)\n",
    "                break\n",
    "                \n",
    "    return answer\n",
    "\n",
    "answer_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpulent :  1\n",
      "intendence :  2\n",
      "validate :  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpulent', 'intendence', 'validate']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "\n",
    "    edit_list = []\n",
    "    for i in range(len(entries)):\n",
    "        edit_dict = {}\n",
    "        for j,word in enumerate(correct_spellings):\n",
    "            edit_dict[word] = nltk.edit_distance(word,entries[i])\n",
    "        edit_list.append(edit_dict)\n",
    "\n",
    "    for i in range(len(edit_list)):\n",
    "        edit_list[i] = {k: v for k, v in sorted(edit_list[i].items(), key = lambda item: item[1])}\n",
    "\n",
    "    answer = []       \n",
    "    for i in range(len(edit_list)):\n",
    "        for key in edit_list[i]:\n",
    "            if key.startswith(entries[i][0]):\n",
    "                print(key,\": \",edit_list[i][key])\n",
    "                answer.append(key)\n",
    "                break\n",
    "                \n",
    "    return answer\n",
    "answer_eleven()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "r35En",
   "launcher_item_id": "tCVfW",
   "part_id": "NTVgL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
